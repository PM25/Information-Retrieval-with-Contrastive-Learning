model:
  LSTM:
      num_layers: 3
      bidirectional: True
      input_size: 768
      hidden_size: 256
      output_size: 128
      activation: "Identity"

loss:
  InfoNCE:
    temperature: 0.05
    use_momentum: True
    momentum: 0.9
    use_queue: True
    queue_size: 12544
    queue_start_steps: 5000

  ProtoNCE:
    temperature: 0.05
    use_momentum: True
    momentum: 0.9
    use_queue: True
    queue_size: 12544
    queue_start_steps: 5000
    cluster_start_steps: 8000
    cluster:
      update_steps: 4000
      num_cluster: [4096, 6144, 8192]
      num_neg_proto: 3072
      verbose: False
      niter: 20
      nredo: 5
      max_points_per_centroid: 1000
      min_points_per_centroid: 1
  
  HProtoNCE:
    temperature: 0.05
    use_momentum: True
    momentum: 0.9
    use_queue: True
    queue_size: 12544
    queue_start_steps: 5000
    cluster_start_steps: 8000
    cluster:
      update_steps: 4000
      num_cluster: [4096, 6144, 8192]
      num_neg_proto: 3072
           
optimizer: 
  SGD:
    learning_rate: "3e-4"
    momentum: 0.9
    weight_decay: "1e-4"
  
  Adam:
    learning_rate: "2.5e-4"                                 
    betas: [0.9, 0.999]

    HProtoNCE:
        temperature: 0.05
        use_momentum: True
        momentum: 0.9
        use_queue: True
        queue_size: 12544
        queue_start_steps: 5000
        cluster_start_steps: 8000
        cluster:
            update_steps: 4000
            num_cluster: [4096, 6144, 8192]
            num_neg_proto: 3072

optimizer:
    SGD:
        learning_rate: "3e-4"
        momentum: 0.9
        weight_decay: "1e-4"

    Adam:
        learning_rate: "2.5e-4"
        betas: [0.9, 0.999]

    gradient_clipping: 1.0 # Maximum gradient norm

    # Training options
train:
    batch_size: 128                                       # training batch size
    acml_batch_size: 256                                  # sample
    apex: False                                           # Use APEX (see https://github.com/NVIDIA/apex for more details)
    total_steps: 100000                                   # total steps for training, a step is a batch of update
    log_step: 1000                                         # evaluate model every this amount of training steps
    n_jobs: 6

# Evaluation options
eval:
    batch_size: 128 # used for dev/test splits
    n_jobs: 3

dataset:
    data_dir: data/fever/
    small_wiki: data/small_wiki.json
    docs_sentence: data/fever/docs_sentence.pkl
    train_data: data/fever/train.jsonl
    dev_data: data/fever/shared_task_dev.jsonl
    docs_dict: data/fever/docs_dict.pkl
    full_wiki: data/full_wiki.json
    full_docs_sentence: data/fever/full_docs_sentence.pkl
    full_docs_dict: data/fever/full_docs_dict.pkl
    full_docs_sentence_similarity: data/fever/full_docs_sentence_similarity.pkl
    inverted_file: data/index/count_matrix.npz
    tfidf: data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz

elastic_search:
    index: wiki
    num_threads: 32
    port: 2021

QA:
  optimizer:
    Adam:
        learning_rate: 1e-5
    warmup_steps: 5000

  # Training options
  train:
      epochs: 3
      log_step: 1
      batch_size: 8 # training batch size
      n_jobs: 30
      freeze_bert: False

  # Evaluation options
  eval:
      batch_size: 8 # used for dev/test splits
      n_jobs: 30
      size: 0.01 # validation data portion

  save: src/QA/models/qa.pth
  seed: 1009
  device_id: 0 # cpu: -1, gpu > 0
    
    